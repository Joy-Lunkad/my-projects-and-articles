# [Replicating Frozen with GPT-J]([https://www.notion.so/Replicating-Frozen-with-GPT-J-7dd8238bda664cf489c7b1187256ffc8?pvs=21](https://joylunkad.notion.site/Replicating-Frozen-with-GPT-J-7dd8238bda664cf489c7b1187256ffc8))

I replicated Deepmind’s [Multimodal Few-Shot Learning with Frozen Language Models](https://arxiv.org/abs/2106.13884). 

I implemented it using GPT-J as the language model and CLIP’s Vision Transformer as the visual encoder. It uses approximately 6.1 Billion Parameters. Using pre-trained models and preemptible TPUv3-8 enabled me to bring down the training cost to a palatable couple of thousand dollars. 

### Input Image

![Untitled](https://github.com/Joy-Lunkad/my-projects-and-articles/assets/63045889/19b36d9f-88a5-441d-acb9-12f7ae200a7d)

### Input Prompt

Question 1: In the picture, what color jerseys are the players wearing? Answer: Green tshirts and black shorts.
Question 2: What are the players doing? Answer:    

### Output Text
Question 1: In the picture, what color jerseys are the players wearing? Answer: Green tshirts and black shorts. 
Question 2: What are the players doing? Answer: They’re warming up for a game

# [Generative Weather forecasting](https://www.notion.so/Generative-Weather-forecasting-5c8573cb95774ef18291397157e5687a?pvs=21)

I designed a novel deep generative 3D-CNN-Transformer Hybrid architecture to build a computationally tractable global climate forecasting engine. 

# [Modelling the thousand brains theory of intelligence](https://www.notion.so/Modelling-the-thousand-brains-theory-of-intelligence-ade9b2db8f2f4f58ad9a4716e7d79285?pvs=21)

I proposed a novel convolutional architecture that improves upon traditional CNNs by building upon Jeff Hawkins’ brilliant thousand brains theory of intelligence.

I presented a soft proof of why convolutions can be used as a foundation for cortical mini-columns. This raises the question that whether the success of CNNs can be used as weak empirical proof for the 1000 brains theory.

# [Breakthrough: Can Giving Memory to Entire Neural Nets be Revolutionary?](https://www.notion.so/Breakthrough-Can-Giving-Memory-to-Entire-Neural-Nets-be-Revolutionary-289d408be8ef4eb2b055de8db66b9da4?pvs=21)

AIM, which stands for AI Memory, is a novel method to attach memory to boost the performance of almost all neural networks. 

It takes only a few additional lines of code to implement and significantly improves a model's ability to learn and generalize. The proof of concept demonstrated that AIM has the potential to improve the performance of every single neural network in the world.
